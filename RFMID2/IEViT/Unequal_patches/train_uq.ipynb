{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.extend([\"..\", \"../../backbone\",\"../..\"])\n",
    "from backbones import DenseNet201b\n",
    "from dataloader import create_dataloader\n",
    "from metric import Metric\n",
    "from ievit_uq import IEViT\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set device to GPU if available, else use CPU\n",
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Current device: {torch.cuda.get_device_name(torch.cuda.current_device())}\" if torch.cuda.is_available() else \"Current device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "learning_rate = 0.000001\n",
    "in_channel = 3\n",
    "image_size = 384\n",
    "# patch_size = 16p\n",
    "patch_size = [32,16,16,16,16,8]\n",
    "num_workers = 4\n",
    "embed_dim = 960\n",
    "mlp_dim = 1024\n",
    "dim_feedforward = 1024\n",
    "num_layers = 12\n",
    "num_labels = 21\n",
    "thresholds = [0.5] * num_labels\n",
    "num_classes = 21\n",
    "data_dir = '../../../../data/GT-main'\n",
    "omit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "../../../../data/GT-main/./set1/train.csv\n",
      "384\n",
      "../../../../data/GT-main/./set1/val.csv\n",
      "384\n",
      "../../../../data/GT-main/./set1/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders (image.shape = torch.Size([batch_size, channel_no, H, W])\n",
    "train_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='train', omit = omit)\n",
    "val_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='val', omit = omit)\n",
    "test_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='test', omit = omit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deependra/project/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/deependra/project/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/deependra/project/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389111445\n"
     ]
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "backbone = DenseNet201b(embed_dim)\n",
    "# create an instance of IEVIT\n",
    "model = IEViT(img_size= image_size, patch_dim = patch_size, in_channels = in_channel, num_classes = num_classes, embed_dim = embed_dim,\n",
    "               num_heads = num_workers, num_layers = num_layers, dim_feedforward = dim_feedforward, mlp_dim = mlp_dim, backbone=backbone)\n",
    "model.to(device)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(num_params)\n",
    "\n",
    "# Wrap your model with DataParallel\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Define the optimizer and the learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, epochs= num_epochs, steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Metric class\n",
    "metric = Metric(num_classes=num_classes)\n",
    "thresholds = None\n",
    "\n",
    "# create empty lists to store predicted probabilities and true labels for each epoch\n",
    "test_preds_all, test_labels_all = [], []\n",
    "\n",
    "# define the epochs at which to plot the ROC curve\n",
    "roc_epochs = [5,10,20,30,40,50,60,70,80,90,100,120,140,160,180,200]\n",
    "\n",
    "# create empty lists to store ROC data for each epoch\n",
    "roc_fpr = []\n",
    "roc_tpr = []\n",
    "roc_auc = []\n",
    "f1_arr = []\n",
    "loss_arr = []\n",
    "model_arr = []\n",
    "max_ms = [0,0,0]\n",
    "\n",
    "# Train and evaluate model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Train phase\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)    \n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad(): metric.update(outputs, labels)\n",
    "    \n",
    "     # Print metrics on train set\n",
    "    ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score, acc_list, spec_list, thresh = metric.compute()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "    print(thresh)\n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}') \n",
    "    # Reset Metric class for evaluation\n",
    "    metric.reset()\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "\n",
    "            # append the predicted probabilities and true labels to lists for calculating ROC AUC score later\n",
    "            val_preds += outputs.tolist()\n",
    "            val_labels += labels.tolist()\n",
    "            \n",
    "            # Compute metrics on validation set\n",
    "            metric.update(outputs, labels)\n",
    " \n",
    "        # Print metrics on validation set\n",
    "        ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score,  acc_list, spec_list, thresh = metric.compute()\n",
    "        print(f\"Val - Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "        \n",
    "        # Reset Metric class for next epoch\n",
    "        metric.reset()\n",
    "        del images\n",
    "        del labels\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()   \n",
    "    print(thresh)   \n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}')\n",
    "    \n",
    "     # Evaluate model on test set\n",
    "    running_loss = 0.0\n",
    "    test_preds, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    " \n",
    "            outputs = model(images)\n",
    "\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            metric.update(outputs, labels)\n",
    "\n",
    "            # append the predicted probabilities and true labels to lists for calculating ROC AUC score later\n",
    "            test_preds += outputs.tolist()\n",
    "            test_labels += labels.tolist()\n",
    " \n",
    "        # Print metrics on test set\n",
    "        ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score,  acc_list, spec_list, thresh = metric.compute(thresholds = thresh)\n",
    "        print(f\"Test - Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "        \n",
    "        f1_arr.append(ml_f1_score)\n",
    "        model_arr.append(model_score)\n",
    "        loss_arr.append(running_loss / len(test_dataloader))\n",
    "        \n",
    "        if max_ms[0] < model_score: \n",
    "            max_ms[0] = model_score\n",
    "            max_ms[1] = epoch\n",
    "            max_ms[2] = optimizer.param_groups[0]['lr']\n",
    "            torch.save(model, 'models/ievit+jseic.pth')\n",
    "\n",
    "        # append the predicted probabilities and true labels for this epoch to the lists for all epochs\n",
    "        test_preds_all.append(test_preds)\n",
    "        test_labels_all.append(test_labels)\n",
    "\n",
    "        # check if the current epoch is in the list of epochs to plot ROC curve\n",
    "        if epoch+1 in roc_epochs:\n",
    "            # calculate ROC curve and AUC score for test set\n",
    "            fpr, tpr, roc_thresholds = roc_curve(np.concatenate(test_labels_all).ravel(), np.concatenate(test_preds_all).ravel())\n",
    "            roc_fpr.append(fpr)\n",
    "            roc_tpr.append(tpr)\n",
    "            roc_auc.append(auc(fpr, tpr))\n",
    "        # Reset Metric class for next epoch\n",
    "        metric.reset()\n",
    "        del images\n",
    "        del labels\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()     \n",
    "    \n",
    "    print(thresh)\n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}') \n",
    "    print()      \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(f1_arr)\n",
    "plt.figure(1)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), f1_arr)\n",
    "plt.plot(range(1,k+1), f1_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1_score\")\n",
    "plt.title('f1_score vs epochs')\n",
    "\n",
    "plt.figure(2)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), loss_arr)\n",
    "plt.plot(range(1,k+1), loss_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Test Loss vs epochs')\n",
    "\n",
    "plt.figure(3)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), f1_arr)\n",
    "plt.plot(range(1,k+1), model_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"model_score\")\n",
    "plt.title('model_score vs epochs')\n",
    "\n",
    "print(max_ms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "fig2, ax2 = plt.subplots()\n",
    "for i in range(len(roc_auc)):\n",
    "    ax2.plot(roc_fpr[i], roc_tpr[i], label=f'ROC curve (epoch {roc_epochs[i]}, area = {roc_auc[i]:.2f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('Receiver Operating Characteristic Curve')\n",
    "# Set the properties for the legend\n",
    "legend = ax2.legend(loc='lower right', bbox_to_anchor=(1.25, 0), fontsize='small', framealpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "model.eval()\n",
    "val_preds, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # append the predicted probabilities and true labels to lists for calculating ROC AUC score later\n",
    "        val_preds += outputs.tolist()\n",
    "        val_labels += labels.tolist()\n",
    "\n",
    "val_labels = np.array(val_labels)\n",
    "val_preds = np.array(val_preds)\n",
    "threshold = 0.5  # set your threshold value here\n",
    "# thresholded_val_preds = np.where(val_preds > threshold, 1, 0)\n",
    "\n",
    "thresholded_val_preds = np.where(val_preds > np.array(thresh).reshape(1, -1), 1, 0)\n",
    "\n",
    "# Calculate the precision, recall, and AUC score for each label using scikit-learn's functions\n",
    "metrics_dict = {}\n",
    "for label in range(num_classes):\n",
    "    precision, recall, _ = precision_recall_curve(val_labels[label], thresholded_val_preds[label])\n",
    "    auc = roc_auc_score(val_labels[label], val_preds[label])\n",
    "    f1 = f1_score(val_labels[label], thresholded_val_preds[label], average='binary', zero_division=1)\n",
    "    metrics_dict[label] = {'Precision': precision, 'Recall': recall, 'F1': f1, 'AUC': auc}\n",
    "    \n",
    "# Print the dictionary in tabular format using the tabulate library\n",
    "headers = ['Label', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "table = []\n",
    "for label in range(num_classes):\n",
    "    row = [label]\n",
    "    for metric in ['Precision', 'Recall', 'F1', 'AUC']:\n",
    "        values = metrics_dict[label][metric]\n",
    "        if values.size > 0:\n",
    "            value_str = '{:.4f}'.format(values.mean(), values.max())\n",
    "        else:\n",
    "            value_str = '0.00'\n",
    "        row.append(value_str)\n",
    "    table.append(row)\n",
    "\n",
    "print(tabulate(table, headers=headers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfmidc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
