{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Current device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../backbone\")\n",
    "from ctran import DenseNet201\n",
    "from dataloader import create_dataloader\n",
    "from metric import Metric\n",
    "from ctran import CTranEncoder\n",
    "from data import RetinaDataset\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set device to GPU if available, else use CPU\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "print(f\"Current device: {torch.cuda.get_device_name(torch.cuda.current_device())}\" if torch.cuda.is_available() else \"Current device: CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# data_dir = '../../../data/GT-main'\n",
    "# split = 'test'  # or 'test' or 'val', depending on the split you want to check\n",
    "# dataset = RetinaDataset(data_dir, split)\n",
    "\n",
    "# black_and_white_images = dataset.get_grayscale_images()\n",
    "# print(\"Black and White Images:\")\n",
    "# print(black_and_white_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs = 200\n",
    "learning_rate = 0.000001\n",
    "image_size = 384\n",
    "num_workers = 4\n",
    "num_labels = 21\n",
    "thresholds = [0.5] * num_labels\n",
    "in_channels = 3\n",
    "num_classes = 21\n",
    "data_dir = '../../../data/GT-main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "../../../data/GT-main/./set1/train.csv\n",
      "384\n",
      "../../../data/GT-main/./set1/val.csv\n",
      "384\n",
      "../../../data/GT-main/./set1/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='train')\n",
    "val_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='val')\n",
    "test_dataloader = create_dataloader(data_dir=data_dir, batch_size=batch_size, num_workers=num_workers, size=image_size, phase='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deependra/project/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/deependra/project/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define model and optimizer\n",
    "backbone = DenseNet201(num_classes=num_classes, embed_dim = 1024)\n",
    "model = CTranEncoder(num_classes=num_classes,  in_channels = 3, embed_dim=1024, num_layers=4, num_heads=num_workers, backbone=backbone)\n",
    "model.to(device)\n",
    "\n",
    "# Wrap your model with DataParallel\n",
    "# model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Define the optimizer and the learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "#scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, epochs= num_epochs, steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid multinomial distribution (encountering probability entry < 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m     28\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1084\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1082\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1117\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1351\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1351\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:620\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/venv/lib/python3.10/site-packages/torch/utils/data/sampler.py:283\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    282\u001b[0m idx_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m    284\u001b[0m     batch[idx_in_batch] \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m    285\u001b[0m     idx_in_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/project/retinal/23-retinal-fundus/RFMID2/C-Tran2/../sampler.py:20\u001b[0m, in \u001b[0;36mWeightedRandomSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (encountering probability entry < 0)"
     ]
    }
   ],
   "source": [
    "# Initialize Metric class\n",
    "metric = Metric(num_classes=num_classes)\n",
    "# thresholds = None\n",
    "\n",
    "# create empty lists to store predicted probabilities and true labels for each epoch\n",
    "test_preds_all, test_labels_all = [], []\n",
    "\n",
    "# define the epochs at which to plot the ROC curve\n",
    "roc_epochs = [5,10,20,30,40,50,60,70,80,90,100,120,140,160,180,200]\n",
    "\n",
    "# create empty lists to store ROC data for each epoch\n",
    "roc_fpr = []\n",
    "roc_tpr = []\n",
    "roc_auc = []\n",
    "f1_arr = []\n",
    "loss_arr = []\n",
    "model_arr = []\n",
    "max_ms = [0,0,0]\n",
    "a=0\n",
    "\n",
    "# Train and evaluate model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('-' * 10)\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)    \n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics on train set\n",
    "        with torch.no_grad(): metric.update(outputs, labels)\n",
    "            \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print metrics on train set\n",
    "    ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score, acc_list, spec_list, thresh = metric.compute(thresholds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "    print(thresh)\n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}') \n",
    "    # Reset Metric class for evaluation\n",
    "    metric.reset()\n",
    "\n",
    "    # Evaluate model on validation set\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    " \n",
    "            outputs = model(images)\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            metric.update(outputs, labels)\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # append the predicted probabilities and true labels to lists for calculating ROC AUC score later\n",
    "            val_preds += outputs.tolist()\n",
    "            val_labels += labels.tolist()\n",
    " \n",
    "        # Print metrics on validation set\n",
    "        ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score,  acc_list, spec_list, thresh = metric.compute(thresholds)\n",
    "        print(f\"Val - Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "        \n",
    "        # Reset Metric class for next epoch\n",
    "        metric.reset()\n",
    "        del images\n",
    "        del labels\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()   \n",
    "    print(thresh)   \n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}') \n",
    "     \n",
    "    # Evaluate model on test set\n",
    "    running_loss = 0.0\n",
    "    test_preds, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    " \n",
    "            outputs = model(images)\n",
    "\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            metric.update(outputs, labels)\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # append the predicted probabilities and true labels to lists for calculating ROC AUC score later\n",
    "            test_preds += outputs.tolist()\n",
    "            test_labels += labels.tolist()\n",
    " \n",
    "        # Print metrics on test set\n",
    "        ml_f1_score, map_score, auc_score, ml_map_score, ml_auc_score, ml_score, bin_auc, model_score, bin_f1_score,  acc_list, spec_list, thresh = metric.compute(thresholds)\n",
    "        print(f\"Test - Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, ML mAP: {ml_map_score:.4f}, ML F1: {ml_f1_score:.4f}, ML AUC: {ml_auc_score:.4f}, ML Score: {ml_score:.4f}, Bin AUC: {bin_auc:.4f}, Model Score: {model_score:.4f}, Bin F1: {bin_f1_score:.4f}\")\n",
    "        \n",
    "        f1_arr.append(ml_f1_score)\n",
    "        model_arr.append(model_score)\n",
    "        loss_arr.append(running_loss / len(test_dataloader))\n",
    "        \n",
    "        if max_ms[0] < model_score: \n",
    "            max_ms[0] = model_score\n",
    "            max_ms[1] = epoch\n",
    "            #torch.save(model, 'models/dn1.pth')\n",
    "\n",
    "        # append the predicted probabilities and true labels for this epoch to the lists for all epochs\n",
    "        test_preds_all.append(test_preds)\n",
    "        test_labels_all.append(test_labels)\n",
    "\n",
    "        # check if the current epoch is in the list of epochs to plot ROC curve\n",
    "        if epoch+1 in roc_epochs:\n",
    "            # calculate ROC curve and AUC score for test set\n",
    "            fpr, tpr, roc_thresholds = roc_curve(np.concatenate(test_labels_all).ravel(), np.concatenate(test_preds_all).ravel())\n",
    "            roc_fpr.append(fpr)\n",
    "            roc_tpr.append(tpr)\n",
    "            roc_auc.append(auc(fpr, tpr))\n",
    "        # Reset Metric class for next epoch\n",
    "        metric.reset()\n",
    "        del images\n",
    "        del labels\n",
    "        del outputs\n",
    "        torch.cuda.empty_cache()     \n",
    "    \n",
    "    print(thresh)\n",
    "    print(f'Accuracy list: {acc_list}')   \n",
    "    print(f'Specificity list: {spec_list}') \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_ms)\n",
    "plt.figure(1)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), f1_arr)\n",
    "plt.plot(range(1,201), f1_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"f1_score\")\n",
    "plt.title('f1_score vs epochs')\n",
    "\n",
    "plt.figure(2)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), loss_arr)\n",
    "plt.plot(range(1,201), loss_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Validation Loss vs epochs')\n",
    "\n",
    "plt.figure(3)\n",
    "#plt.plot(sorted(list(set(roc_epochs))), f1_arr)\n",
    "plt.plot(range(1,201), model_arr)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"model_score\")\n",
    "plt.title('model_score vs epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "fig2, ax2 = plt.subplots()\n",
    "for i in range(len(roc_auc)):\n",
    "    ax2.plot(roc_fpr[i], roc_tpr[i], label=f'ROC curve (epoch {roc_epochs[i]}, area = {roc_auc[i]:.2f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('Receiver Operating Characteristic Curve')\n",
    "# Set the properties for the legend\n",
    "legend = ax2.legend(loc='lower right', bbox_to_anchor=(1.25, 0), fontsize='small', framealpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfmidc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
